{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ae182f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers, AutoTokenizer, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "190f9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11e6a57a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a3c5eb2e494db99949eae495bdc482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea259d4b3d674f6eb5a6779346327454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5e846e3967b4e44b6c0e24179d69726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b01eb09a94a4bc2850086e01990a421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ea9d0c300e427290787a427b8fb033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3584a0839344468a955501248d7ac24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b2187004da46318133571fc3a81281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in dataset: 36718\n"
     ]
    }
   ],
   "source": [
    "# 1. Load a text dataset (we use a small example dataset for demonstration)\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")  # raw text WikiText-2\n",
    "print(f\"Number of lines in dataset: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bb52a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize a tokenizer (we'll use GPT-2's tokenizer for compatibility with a GPT-2 model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-2 doesn't have a pad by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0cd45b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9211013e3845b0980346abf3dd6647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# 3. Tokenize the dataset efficiently using `.map` with batched processing\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=False)\n",
    "\n",
    "tokenized_ds = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "# The dataset now has columns like 'input_ids' and 'attention_mask'\n",
    "\n",
    "print(tokenized_ds[0][\"input_ids\"][:20])  # print first 20 token IDs of first example for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe849645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d900915b9aa460e97cea19eb134026e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM training sequences: 18667\n"
     ]
    }
   ],
   "source": [
    "# 4. Slice into training sequences of fixed length\n",
    "# For language model training, often we concatenate all texts then split into blocks of e.g. 128 or 512 tokens.\n",
    "block_size = 128\n",
    "def group_texts(examples):\n",
    "    # Concatenate each field\n",
    "    concatenated_inputs = sum(examples[\"input_ids\"], [])\n",
    "    concatenated_masks = sum(examples[\"attention_mask\"], [])\n",
    "\n",
    "    total_len = (len(concatenated_inputs) // block_size) * block_size\n",
    "    concatenated_inputs = concatenated_inputs[:total_len]\n",
    "    concatenated_masks = concatenated_masks[:total_len]\n",
    "\n",
    "    # Split into chunks\n",
    "    result_input_ids = [concatenated_inputs[i:i+block_size] for i in range(0, total_len, block_size)]\n",
    "    result_masks = [concatenated_masks[i:i+block_size] for i in range(0, total_len, block_size)]\n",
    "\n",
    "    return {\"input_ids\": result_input_ids, \"attention_mask\": result_masks}\n",
    "\n",
    "\n",
    "lm_ds = tokenized_ds.map(group_texts, batched=True, batch_size=1000)\n",
    "print(f\"LM training sequences: {len(lm_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a5f9cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Create a DataLoader for the tokenized, grouped dataset\n",
    "# We'll use a custom collate to dynamically pad sequences (though all are same length here by construction)\n",
    "def collate_fn(batch):\n",
    "    # Since our sequences are fixed length after grouping, we might just stack them.\n",
    "    # If they weren't fixed, we could use tokenizer.pad to pad to max length in batch.\n",
    "    input_ids = torch.tensor([example[\"input_ids\"] for example in batch], dtype=torch.long)\n",
    "    # For language modeling, labels are the input_ids shifted by one, but \n",
    "    # Transformers' CausalLM models usually handle that internally if we provide labels = input_ids.\n",
    "    return {\"input_ids\": input_ids, \"labels\": input_ids.clone()}\n",
    "\n",
    "train_loader = DataLoader(lm_ds, batch_size=8, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5edc38fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128]) torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "# 6. Iterate through a couple of batches to see that it works\n",
    "for batch in train_loader:\n",
    "    print(batch[\"input_ids\"].shape, batch[\"labels\"].shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
